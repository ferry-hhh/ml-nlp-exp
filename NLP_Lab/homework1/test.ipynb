{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 两种读取txt文件的方法对比"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "f1 = open(\"./dict.txt\",\"r\", encoding=\"utf8\")\r\n",
    "lines1 = f1.read() # 读到str类型\r\n",
    "print(type(lines1))\r\n",
    "print(\"lines1 =\", lines1)\r\n",
    "\r\n",
    "f2 = open(\"./dict.txt\",\"r\", encoding=\"utf8\")\r\n",
    "lines2 = f2.readlines() # 读到list类型\r\n",
    "print(type(lines2))\r\n",
    "print(\"lines2 =\", lines2)\r\n",
    "# 使用readlines读取到list可以进行逐行处理\r\n",
    "for line in lines2:\r\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'str'>\n",
      "lines1 = 江泽民\n",
      "国家主席\n",
      "1998年\n",
      "1997年\n",
      "1995年\n",
      "1999年\n",
      "<class 'list'>\n",
      "lines2 = ['江泽民\\n', '国家主席\\n', '1998年\\n', '1997年\\n', '1995年\\n', '1999年']\n",
      "江泽民\n",
      "\n",
      "国家主席\n",
      "\n",
      "1998年\n",
      "\n",
      "1997年\n",
      "\n",
      "1995年\n",
      "\n",
      "1999年\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 为每个句子添加头BOS，与尾EOS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import jieba\r\n",
    "import re\r\n",
    "\r\n",
    "#语料句子\r\n",
    "sentence_ori=\"\"\"研究生物很有意思。他大学时代是研究生物的。生物专业是他的首选目标。（他是研究生。） \r\n",
    "他一直很喜欢生物。生物专业真的很不错。\"\"\"\r\n",
    "#测试句子\r\n",
    "\r\n",
    "sentence_test=\"研究生物专业是他的首选目标\"\r\n",
    "\r\n",
    "#任务：完成对2-gram模型的建立，计算测试句子概率并输出结果\r\n",
    "\r\n",
    "punc = r\"\"\"\\n ！  ？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”？，！【】（）、。：；’‘……￥·\"\"\"\r\n",
    "\r\n",
    "def Modify(s):\r\n",
    "    #将结尾标点符号截掉\r\n",
    "    if s[-1] in (r\"[%s]+\"%punc):\r\n",
    "        s = s[:-1]  #截取字符串从开头到倒数一个字符的子串\r\n",
    "    if s[0] in (r\"[%s]+\"%punc):\r\n",
    "        s = s[1:]\r\n",
    "    #添加起始符BOS和终止符EOS   \r\n",
    "    s_modify1 = re.sub(r\"[%s]+\"%punc, \"EOS BOS\", s)   ## r'\\w+'为正则表达式，匹配多个英文单词或者数字  \r\n",
    "    s_modify2=\"BOS\"+s_modify1+\"EOS\"\r\n",
    "    return s_modify2\r\n",
    "\r\n",
    "train_Modifys=Modify(sentence_ori)\r\n",
    "print(train_Modifys)\r\n",
    "\r\n",
    "test_Modifys =Modify(sentence_test)\r\n",
    "# print(test_Modifys)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BOS研究生物很有意思EOS BOS他大学时代是研究生物的EOS BOS生物专业是他的首选目标EOS BOS他是研究生EOS BOS他一直很喜欢生物EOS BOS生物专业真的很不错EOS\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 博主所用分词方法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def Partition_Statistics(s, dicts = None):\r\n",
    "    jieba.suggest_freq(('BOS','EOS'), True)#分开\r\n",
    "    s = jieba.cut(s, HMM = False)  #精确模式，自动计算的词频在使用 HMM 新词发现功能时可能无效,所以设为False\r\n",
    "    format_s = \",\".join(s)\r\n",
    "    #将词按\",\"分割后依次填入数组\r\n",
    "    lists = [i.strip() for i in format_s.split(\",\") if i not in punc]\r\n",
    "    # s = jieba.lcut(s, HMM = False)\r\n",
    "    #统计词频\r\n",
    "    if dicts != None:\r\n",
    "        for word in lists:\r\n",
    "            if word not in dicts:\r\n",
    "                dicts[word] = 1\r\n",
    "            else:\r\n",
    "                dicts[word] += 1               \r\n",
    "    return lists , dicts\r\n",
    "\r\n",
    "\r\n",
    "train_seg,train_count = Partition_Statistics(train_Modifys,dicts = {})\r\n",
    "print(train_seg)\r\n",
    "print(train_count)\r\n",
    "test_seg,test_count = Partition_Statistics(test_Modifys,dicts = {})\r\n",
    "print(test_seg)\r\n",
    "print(test_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['BOS', '研究', '生物', '很', '有意思', 'EOS', 'BOS', '他', '大学', '时代', '是', '研究', '生物', '的', 'EOS', 'BOS', '生物', '专业', '是', '他', '的', '首选', '目标', 'EOS', 'BOS', '他', '是', '研究生', 'EOS', 'BOS', '他', '一直', '很', '喜欢', '生物', 'EOS', 'BOS', '生物', '专业', '真的', '很', '不错', 'EOS']\n",
      "{'BOS': 6, '研究': 2, '生物': 5, '很': 3, '有意思': 1, 'EOS': 6, '他': 4, '大学': 1, '时代': 1, '是': 3, '的': 2, '专业': 2, '首选': 1, '目标': 1, '研究生': 1, '一直': 1, '喜欢': 1, '真的': 1, '不错': 1}\n",
      "['BOS', '研究', '生物', '专业', '是', '他', '的', '首选', '目标', 'EOS']\n",
      "{'BOS': 1, '研究': 1, '生物': 1, '专业': 1, '是': 1, '他': 1, '的': 1, '首选': 1, '目标': 1, 'EOS': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 更简单的分词同时加入自定义词典"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def WordCount(s):\r\n",
    "    jieba.load_userdict(\"./dict.txt\")#加载自定义字典\r\n",
    "    jieba.suggest_freq(('BOS','EOS'), True) # 分开\r\n",
    "    word_list = jieba.lcut(s, cut_all=False)  # 分词\r\n",
    "    while ' ' in word_list:\r\n",
    "        word_list.remove(' ')\r\n",
    "    #统计词频\r\n",
    "    dicts = {}\r\n",
    "    for word in word_list:\r\n",
    "        if word not in dicts:\r\n",
    "            dicts[word] = 1\r\n",
    "        else:\r\n",
    "            dicts[word] += 1\r\n",
    "    return word_list , dicts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def CompareList(ori_list,test_list):\r\n",
    "    #申请空间\r\n",
    "    count_list=[0]*(len(test_list)-1)\r\n",
    "    #遍历测试的字符串\r\n",
    "    for i in range(0, len(test_list)-1):\r\n",
    "        #遍历语料字符串，因为是二元语法，不用比较语料字符串的最后一个字符\r\n",
    "        for j in range(0,len(ori_list)):\r\n",
    "            #如果测试的第一个词和语料的第一个词相等则比较第二个词\r\n",
    "            if test_list[i]==ori_list[j]:\r\n",
    "                if test_list[i+1]==ori_list[j+1]:\r\n",
    "                    print(j)\r\n",
    "                    count_list[i]+=1\r\n",
    "    return count_list\r\n",
    "\r\n",
    "def Probability(test_list,count_list,ori_dict):\r\n",
    "    #概率值为p\r\n",
    "    p=1\r\n",
    "    for i in range(len(count_list)): \r\n",
    "        p *=(float(count_list[i])/float(ori_dict[test_list[i]]))\r\n",
    "    return p\r\n",
    "\r\n",
    "\r\n",
    "count_list = CompareList(train_seg, test_seg)\r\n",
    "print(count_list)\r\n",
    "p = Probability(train_seg,count_list,train_count)\r\n",
    "print(p)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "11\n",
      "16\n",
      "37\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "[1, 2, 2, 1, 1, 1, 1, 1, 1]\n",
      "0.00015432098765432096\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import re\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def removePunctuation(sentence_str):\r\n",
    "    '''给定字符串，进行特殊符号过滤，以及小写转换\r\n",
    "    Args:\r\n",
    "        sentence_str (str): 句子\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        sentence_str.lower() (str): 预处理过的句子\r\n",
    "    '''\r\n",
    "    punctuation = '.!,;:?\\-\"\\''\r\n",
    "    sentence_str = re.sub(r'[{}]+'.format(punctuation),'',sentence_str)\r\n",
    "    return sentence_str.lower()\r\n",
    "\r\n",
    "def get_sentence(filename):\r\n",
    "    '''从给定文件中获取句子\r\n",
    "    Args:\r\n",
    "        filename (str): 语料库的文件名\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        sentences_list (list): 1-D，存放了所有经过预处理后的句子\r\n",
    "    '''\r\n",
    "    sentences_list = []\r\n",
    "    with open(filename, encoding='utf-8') as f:\r\n",
    "        for line in f.readlines():\r\n",
    "            sentence_str = removePunctuation(line.strip().split('|')[-1])\r\n",
    "            sentences_list.append('<s> ' + sentence_str + ' </s>')\r\n",
    "    return sentences_list\r\n",
    "\r\n",
    "def count_word(sentences_list):\r\n",
    "    '''给定大量句子，统计出所有单词出现的频次\r\n",
    "    Args:\r\n",
    "        sentences_list (list): 所有经过预处理后的句子\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        wordcount_dict (dict): 键是str类型，表示单词；值是int类型，表示次数，例如{'the': 1234}\r\n",
    "    '''\r\n",
    "    wordcount_dict = {} \r\n",
    "    for sentence_str in sentences_list:\r\n",
    "        for word in sentence_str.split():\r\n",
    "            if word in wordcount_dict:\r\n",
    "                wordcount_dict[word] += 1\r\n",
    "            else:\r\n",
    "                wordcount_dict[word] = 1\r\n",
    "    return wordcount_dict\r\n",
    "\r\n",
    "def word2idx(wordcount_dict):\r\n",
    "    '''构建单词到索引的映射与逆映射\r\n",
    "    Args:\r\n",
    "        wordcount_dict (dict): 键是str类型，表示单词；值是int类型，表示次数\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        word2idx_dict (dict): 键是str类型，表示单词；值是int类型，表示索引，例如{'the': 0}\r\n",
    "        idx2word_dict (dict): 键是int类型，表示索引；值是str类型，表示单词，例如{0: 'the'}\r\n",
    "    '''\r\n",
    "    word2idx_dict = {}\r\n",
    "    idx2word_dict = {}\r\n",
    "    for idx, word in enumerate(list(wordcount_dict.keys())):\r\n",
    "        word2idx_dict[word] = idx\r\n",
    "        idx2word_dict[idx] = word\r\n",
    "    return word2idx_dict, idx2word_dict\r\n",
    "\r\n",
    "def c_table(word2idx_dict, sentences_list, smooth=False):\r\n",
    "    '''构建两个单词连续出现的频次矩阵\r\n",
    "    Args:\r\n",
    "        word2idx_dict (dict): 键是str类型，表示单词；值是int类型，表示索引\r\n",
    "        sentences_list (list): 所有经过预处理后的句子\r\n",
    "        smooth (bool): 是否进行加一平滑\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        c_table_np (numpy): 2-D，c_table_np[i][j] = a表示 前一个索引为i的词和当前索引为j的词 同时出现的次数为a\r\n",
    "    '''\r\n",
    "    n = len(word2idx_dict) # 单词个数\r\n",
    "    c_table_np = np.zeros((n, n)) # n*n 全0矩阵\r\n",
    "    for sentence_str in sentences_list:\r\n",
    "        words_list = sentence_str.split() # ['i', 'like', 'apple']\r\n",
    "        for i in range(1, len(words_list)):\r\n",
    "            w_i = word2idx_dict[words_list[i]] # w_i\r\n",
    "            w_j = word2idx_dict[words_list[i-1]] # w_{i-1}\r\n",
    "            c_table_np[w_j][w_i] += 1\r\n",
    "    \r\n",
    "    if smooth: # 加一平滑\r\n",
    "        c_table_np[c_table_np == 0] = 1\r\n",
    "    \r\n",
    "    return c_table_np\r\n",
    "\r\n",
    "def compute_bigram_table(c_table_np, wordcount_dict):\r\n",
    "    '''构建bigram概率矩阵\r\n",
    "    Args:\r\n",
    "        c_table_np (numpy): bigram频次矩阵\r\n",
    "        wordcount_dict (dict): 所有单词出现的次数\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        c_table_np / count_np[:, None] (numpy): 2-D，bigram概率矩阵\r\n",
    "    '''\r\n",
    "    count_np = np.array(list(wordcount_dict.values())) # [800, 900, 788, ...]\r\n",
    "    return c_table_np / count_np[:, None]\r\n",
    "\r\n",
    "def compute_sentence_bigram(bigram_table_np, word2idx_dict, sentences_list):\r\n",
    "    '''计算每个句子的bigram概率\r\n",
    "    Args:\r\n",
    "        bigram_table_np (numpy): bigram概率矩阵\r\n",
    "        word2idx_dict (dict): 单词到索引的映射\r\n",
    "        sentences_list (list): 预处理后的句子\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        scores_list (list): 所有句子的bigram概率\r\n",
    "    '''\r\n",
    "    scores_list = []\r\n",
    "    for sentence_str in sentences_list:\r\n",
    "        words_list = sentence_str.split()\r\n",
    "        score = 1\r\n",
    "        for i in range(1, len(words_list)):\r\n",
    "            w_i = word2idx_dict[words_list[i]] # w_i\r\n",
    "            w_j = word2idx_dict[words_list[i-1]] # w_{i-1}\r\n",
    "            score *= bigram_table_np[w_j][w_i]\r\n",
    "        scores_list.append(score)\r\n",
    "    return scores_list\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    sentences_list = get_sentence('./finish.txt')\r\n",
    "    wordcount_dict = count_word(sentences_list)\r\n",
    "    word2idx_dict, idx2word_dict = word2idx(wordcount_dict)\r\n",
    "    c_table_np = c_table(word2idx_dict, sentences_list, True)\r\n",
    "    bigram_table_np = compute_bigram_table(c_table_np, wordcount_dict)\r\n",
    "    scores_list = compute_sentence_bigram(bigram_table_np, word2idx_dict, sentences_list)\r\n",
    "    print(scores_list[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05, 4.3357613596947624e-05]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import jieba\r\n",
    "import re\r\n",
    "#语料句子\r\n",
    "sentence_ori=\"研究生物很有意思。他大学时代是研究生物的。生物专业是他的首选目标。他是研究生。\"\r\n",
    "#测试句子\r\n",
    "\r\n",
    "sentence_test=\"明确而言，国家法律对这种擅自拆除承重墙装修的当事人应受到何种行政处罚和民事责任均有规定。\"\r\n",
    "\r\n",
    "#任务：完成对2-gram模型的建立，计算测试句子概率并输出结果\r\n",
    "\r\n",
    "\r\n",
    "punc = r\"\"\"！  ？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”？，！【】（）、。：；’‘……￥·\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "punctuation_map = dict((ord(char), \"EOSBOS\") for char in punc)  \r\n",
    "new_s=sentence_ori.translate(punctuation_map)\r\n",
    "print(new_s)\r\n",
    "\r\n",
    "\r\n",
    "def Modify(s):\r\n",
    "    #将结尾标点符号截掉\r\n",
    "    if s[-1] in (r\"[%s]+\"%punc):\r\n",
    "        s = s[:-1]  #截取字符串从开头到倒数一个字符的子串\r\n",
    "\r\n",
    "    #添加起始符BOS和终止符EOS   \r\n",
    "    s_modify1 = re.sub(r\"[%s]+\"%punc, \"EOS BOS\", s)   ## r'\\w+'为正则表达式，匹配多个英文单词或者数字  \r\n",
    "    s_modify2=\"BOS\"+s_modify1+\"EOS\"\r\n",
    "    return s_modify2\r\n",
    "\r\n",
    "train_Modifys=Modify(sentence_ori)\r\n",
    "print(train_Modifys)\r\n",
    "\r\n",
    "test_Modifys =Modify(sentence_test) \r\n",
    "\r\n",
    "def Partition_Statistics(s, dicts = None):\r\n",
    "    jieba.suggest_freq(('BOS','EOS'), True)#分开\r\n",
    "    s = jieba.cut(s, HMM = False)  #精确模式，自动计算的词频在使用 HMM 新词发现功能时可能无效,所以设为False\r\n",
    "    format_s = \",\".join(s)\r\n",
    "    #将词按\",\"分割后依次填入数组\r\n",
    "    lists = [i.strip() for i in format_s.split(\",\") if i  not in punc ]\r\n",
    "    #统计词频\r\n",
    "    if dicts != None:\r\n",
    "        for word in lists:\r\n",
    "            if word not in dicts:\r\n",
    "                dicts[word] = 1\r\n",
    "            else:\r\n",
    "                dicts[word] += 1               \r\n",
    "    return lists , dicts\r\n",
    "\r\n",
    "\r\n",
    "train_seg,train_count = Partition_Statistics(train_Modifys,dicts = {})\r\n",
    "print(train_seg  )\r\n",
    "print(train_count)\r\n",
    "test_seg,test_count = Partition_Statistics(test_Modifys,dicts = {})\r\n",
    "\r\n",
    "def CompareList(ori_list,test_list):\r\n",
    "    #申请空间\r\n",
    "    count_list=[0]*(len(test_list)-1)\r\n",
    "    #遍历测试的字符串\r\n",
    "    for i in range(0, len(test_list)-1):\r\n",
    "        #遍历语料字符串，因为是二元语法，不用比较语料字符串的最后一个字符\r\n",
    "        for j in range(0,len(ori_list)-1):\r\n",
    "            #如果测试的第一个词和语料的第一个词相等则比较第二个词\r\n",
    "            if test_list[i]==ori_list[j]:\r\n",
    "                if test_list[i+1]==ori_list[j+1]:\r\n",
    "                    count_list[i]+=1\r\n",
    "    return count_list\r\n",
    "\r\n",
    "def Probability(test_list,count_list,ori_dict):\r\n",
    "    #概率值为p\r\n",
    "    p=1\r\n",
    "    for i in range(len(count_list)): \r\n",
    "        p *=(float(count_list[i])/float(ori_dict[test_list[i]]))\r\n",
    "    return p\r\n",
    "\r\n",
    "\r\n",
    "count_list = CompareList(train_seg, test_seg)\r\n",
    "print(count_list)\r\n",
    "p = Probability(train_seg,count_list,train_count)\r\n",
    "print(p)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "研究生物很有意思EOSBOS他大学时代是研究生物的EOSBOS生物专业是他的首选目标EOSBOS他是研究生EOSBOS\n",
      "BOS研究生物很有意思EOS BOS他大学时代是研究生物的EOS BOS生物专业是他的首选目标EOS BOS他是研究生EOS\n",
      "['BOS', '研究', '生物', '很', '有意思', 'EOS', 'BOS', '他', '大学', '时代', '是', '研究', '生物', '的', 'EOS', 'BOS', '生物', '专业', '是', '他', '的', '首选', '目标', 'EOS', 'BOS', '他', '是', '研究生', 'EOS']\n",
      "{'BOS': 4, '研究': 2, '生物': 3, '很': 1, '有意思': 1, 'EOS': 4, '他': 3, '大学': 1, '时代': 1, '是': 3, '的': 2, '专业': 1, '首选': 1, '目标': 1, '研究生': 1}\n",
      "[0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('py38': conda)"
  },
  "interpreter": {
   "hash": "657e090079c3b878f34aa47a5d22d2b5ee860e49daf290df837fef2eaed85ca4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}